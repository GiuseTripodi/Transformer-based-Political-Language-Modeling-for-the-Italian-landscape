{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentence Similarity\n\n**Author:** [Giuseppe Tripodi](https://www.linkedin.com/in/giuseppe-tripodi-unical/)<br>\n**Date created:** 2022/11/12<br>\n**Description:** Sentence Similarity between electoral program","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"## Install package","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers\n!pip install sentencepiece\n!pip install sacremoses\n!pip install transformers\n!pip install evaluate\n!pip install wandb\n!pip install -U sentence-transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport csv\nimport re\nimport wandb\nimport transformers\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, \\\n    EarlyStoppingCallback\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer\nfrom sklearn import preprocessing\nimport numpy as np\nimport evaluate\nfrom transformers.integrations import TensorBoardCallback\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n#load dataset\nfrom datasets import load_dataset, load_metric\n#tockenizer\nfrom transformers import AutoTokenizer\nfrom transformers import Pipeline, TextClassificationPipeline\nimport numpy as np\nfrom datasets import load_dataset, load_metric\nimport pandas as pd\nimport torch\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sentence_transformers import SentenceTransformer, util\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom os import listdir\nfrom os.path import isfile, join\nfrom datetime import date\nimport sklearn\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport heapq\nimport nltk\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup Weight&Biases and Variables","metadata":{}},{"cell_type":"code","source":"today = date.today()\ntoday = today.strftime(\"%b-%d-%Y\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env WANDB_PROJECT=\n%env WANDB_LOG_MODEL=\n%env WANDB_API_KEY=\n\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Functions","metadata":{}},{"cell_type":"code","source":"def join_csv(input_dir, output_name):\n    files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n    li = []\n    indexes = []\n    for file in files:\n        df = pd.read_csv(join(input_dir, file), index_col=None, header=0)\n        index = file[:file.index(\".\")]\n        li.append(df)\n        indexes.append(index)\n\n    frame = pd.concat(li, axis=0, ignore_index=True)\n    frame[\"indexes\"] = indexes\n    frame.set_index(\"indexes\", inplace=True)\n    \n    #save the output as a csv file\n    frame.to_csv(output_name)\n    \n    return frame","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sentence Similarity","metadata":{}},{"cell_type":"code","source":"class SentenceSimilarity:\n    \"\"\"\n    A class used to compute the similarity between two sentences\n    \"\"\"\n\n    def __init__(self, sentence_transformer: str):\n        \"\"\"\n        Parameters\n        -------------\n        :parm sentence_transformer: str\n            sentence transformer used to compute the similarity\n        \"\"\"\n        # initialize the sentence transformer\n        self.model = SentenceTransformer(sentence_transformer)\n\n    def similarity(self, sentences1: [], sentences2: []):\n        \"\"\"\n        Does and return the similarity between the sentences\n        :return:\n        \"\"\"\n        # compute embedding for both texts\n        embedding_text1 = self.model.encode(sentences1, convert_to_tensor=True, show_progress_bar=False)\n        embedding_text2 = self.model.encode(sentences2, convert_to_tensor=True, show_progress_bar=False)\n\n        # compute the similarity\n        return util.pytorch_cos_sim(embedding_text1, embedding_text2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def programs_similarity(df_programs: pd.DataFrame, ss: SentenceSimilarity, program1: str, program2: str,\n                        arguments: []) -> float:\n    if program1 == program2:\n        return 1\n    sentence_program1 = df_programs.loc[program1][arguments].values\n    sentence_program2 = df_programs.loc[program2][arguments].values\n    cosine_scores = ss.similarity(sentence_program1, sentence_program2)\n    sum = 0\n    for i in range(len(sentence_program1)):\n        sum += cosine_scores[i][i].item()\n    return sum / len(arguments)\n\n\ndef similarity_matrix(df_programs: pd.DataFrame, arguments: [], model='all-MiniLM-L6-v2') -> pd.DataFrame:\n    \"\"\"\n    The method computes the similarity matrix between every program and return it as a dataframe.\n    The similarity is computed between every program,\n    but it is done by considering only the argument in the array arguments.\n\n    if arguments = [\"Lavoro\", \"Diritti\"], it is computed the similarity between the program of every politician but only\n    considering the two indicated argument. So it is done the mean between the two results.\n    \"\"\"\n    matrix = []\n    ss = SentenceSimilarity(model)\n    for program in df_programs.index:\n        similarity_program = []\n        for program2 in df_programs.index:\n            sim = programs_similarity(df_programs, ss, program, program2, arguments)\n            similarity_program.append(sim)\n        matrix.append(similarity_program)\n    return pd.DataFrame(matrix, index=df_programs.index, columns=df_programs.index)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def most_common_words(text: str, top_n) -> str:\n    \"\"\"\n    Gets a text and return an extractive summarization of the text\n\n    code from: https://stackabuse.com/text-summarization-with-nltk-in-python/\n    \"\"\"\n    # preprocessing\n    # Removing Square Brackets and Extra Spaces\n    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Removing special characters and digits\n    formatted_article_text = re.sub('[^a-zA-Z]', ' ', text)\n    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n\n    # converting text to sentences\n    sentence_list = sent_tokenize(text, language=\"italian\")\n\n    # Find Weighted Frequency of Occurrence\n    stopwords = nltk.corpus.stopwords.words('italian')\n\n    word_frequencies = {}\n    for word in nltk.word_tokenize(formatted_article_text):\n        if word not in stopwords:\n            if word not in word_frequencies.keys():\n                word_frequencies[word] = 1\n            else:\n                word_frequencies[word] += 1\n    # divide the number of occurances of all the words by the frequency of the most occurring word\n    maximum_frequncy = max(word_frequencies.values())\n\n    for word in word_frequencies.keys():\n        word_frequencies[word] = (word_frequencies[word] / maximum_frequncy)\n        \n    # retrieves top_n words and return it\n    most_common_words = heapq.nlargest(top_n, word_frequencies, key=word_frequencies.get)\n    return most_common_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"Piano nazionale per dotare tutti gli edifici pubblici di impianti fotovoltaici e alleanza con mondo agricolo per installazione impianti fotovoltaici ed eolici. Sostegno e promozione dell’economia circolare sui rifiuti quale modello per riutilizzare e riciclare materiali e prodotti trasformando i rifiuti indifferenziati in energia e i differenziati in materia prima. Semplificazione e incentivi strutturali per il potenziamento e l'estensione di tutti gli impianti rinnovabili nazionali. Investimenti per supportare la realizzazione di impianti per le energie rinnovabili (energia eolica, solare, idroelettrica e pelagica, geotermica e bioenergia). Sostegno e incentivi all’innovazione digitale per la tracciabilità dei rifiuti attraverso l’utilizzo dei nuovi sistemi di Blockchain. Potenziamento della semplificazione, di incentivi strutturali e crediti di imposta per le imprese che riconvertono e investono in eco innovazione e nuove tecnologie. SÌ ad una transizione ecologica e energetica giusta, basata su uno sviluppo sostenibile che tuteli l’ambiente attraverso il sostegno alla ricerca e all’innovazione tecnologica. Sì ai termovalorizzatori e agli impianti a biomassa per il recupero totale dei rifiuti indifferenziati e degli scarti agricoli e forestali a fini energetici. Promozione di una gestione produttiva e sostenibile del patrimonio forestale e arboreo urbano, incrementandolo con la piantumazione, rispetto a quanto già previsto, di 1 milione ulteriore di alberi nuovi. Semplificazione per l’installazione di impianti fotovoltaici sugli edifici privati.\"\nmost_common_words(text, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def return_most_common_words_by_arguments(df:pd.DataFrame, arguments: [], top_n):\n    ret = []\n    for argument in arguments:\n        print(f\"Argument: {argument}\")\n        for pol in df.index:\n            ret.append({\"pol\": pol, \"words\":most_common_words(df.loc[pol][argument], top_n)})\n        dataframe = pd.DataFrame(ret).set_index(\"pol\")\n        dataframe.to_csv(f\"{argument}.csv\")\n    return dataframe\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Functions","metadata":{}},{"cell_type":"code","source":"def plot_similarity_matrix(similarity_matrix, title:str):\n    \"\"\"\n    Plots the general similarity matrix\n    \"\"\"\n    disp = ConfusionMatrixDisplay(confusion_matrix=similarity_matrix.to_numpy(),  display_labels=similarity_matrix.index)\n\n    disp.plot(cmap=plt.cm.Reds)\n    disp.ax_.set_title(title, fontsize=17)\n    disp.ax_.tick_params(axis='x', which='major', labelsize=13)\n    disp.ax_.tick_params(axis='y', which='major', labelsize=13)\n    disp.figure_.set_figwidth(13)\n    disp.figure_.set_figheight(10)\n    plt.xticks(rotation=30)\n    plt.savefig(f\"{title}_{today}.png\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_similarity_matrix_by_argument(df_programs:pd.DataFrame, arguments: [], model:str):\n    \"\"\"\n    plots the similarity matrix separately for every argument in arguments\n    \"\"\"\n    for argument in arguments:\n        cm = similarity_matrix(df_programs, [argument] , model)\n        plot_similarity_matrix(cm, f\"Similarità categoria: {argument}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_scatter_plot(df_programs:pd.DataFrame, model:str):\n    \"\"\"\n    compute the scatter plot of the embedding of the full program for every politician\n    \"\"\"\n    model = SentenceTransformer(model)\n    tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3, random_state = 49)\n    df_programs[\"program\"] = df_programs[df_programs.columns].apply(\"_\".join, axis=1)\n    le = preprocessing.LabelEncoder()\n    title = \"Embedding Scatter Plot\"\n    \n    #create embedding for arguments\n    embedding = []\n    labels = []\n    columns = []\n    for column in df_programs.columns:\n        for index in df_programs.index:\n            text = df_programs.loc[index][column]\n            embedding.append(model.encode(text, convert_to_tensor=False, show_progress_bar=False))\n            labels.append(index)\n            columns.append(column)\n    #create the dataframe with the embedding\n    embedding = pd.DataFrame(embedding)\n    embedding = tsne.fit_transform(embedding)\n    embedding = pd.DataFrame(embedding, columns=[\"X\", \"Y\"])\n    embedding[\"labels\"] = labels\n    embedding[\"category\"] = columns\n    \n    # create map for labels\n    labels = [\"o\",\"v\", \"1\", \">\",\"s\",\"+\",\"x\",\"D\",\"X\",\"3\", \"H\", \"D\"]\n    map_labels = {}\n    for column in range(len(df_programs.columns)):\n        map_labels[df_programs.columns[column]] = labels[column]\n                   \n            \n    # create map for colors\n    colors = ['tab:blue','tab:orange','tab:green','tab:red','tab:purple','tab:brown']\n    map_colors = {}\n    for index in range(len(df_programs.index)):\n         map_colors[df_programs.index[index]] = colors[index]\n\n    N = len(df_programs.index)\n    x = embedding[\"X\"]\n    y = embedding[\"Y\"]\n    colors = embedding[\"labels\"].map(map_colors).values\n    markers = embedding[\"category\"].map(map_labels).values\n    area = [300 if embedding.loc[i][\"category\"] != \"program\" else 3000 for i in embedding.index]\n    fig = plt.figure(figsize=(15,10))\n    plot_lines = []\n    for i in range(len(embedding.index)):\n        l = plt.scatter(x[i], y[i], s=area[i], c=colors[i], marker=markers[i], alpha=0.5)\n        plot_lines.append([l, embedding[\"category\"][i]])\n    plot_lines = pd.DataFrame(plot_lines, columns=[\"lines\", \"category\"])\n    legend1 = plt.legend(plot_lines[\"lines\"].values[0:-6:6], plot_lines[\"category\"].values[0:-6:6],markerscale=0.5, loc='upper left', bbox_to_anchor=(1, 1))\n    plt.gca().add_artist(legend1)\n    plt.legend(plot_lines[\"lines\"].values[-6:], df_programs.index ,markerscale=0.2, bbox_to_anchor=(1, 0.5), loc='upper left')\n    plt.title(title, fontsize=20)\n    plt.savefig(f\"{title}_{today}.png\")\n    \n    plt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute Sentence Similarity","metadata":{}},{"cell_type":"code","source":"# Define some variables\n#input_dir = \"/kaggle/input/electoral-programs/summ_by_concept\" # sum by online information\ninput_dir = \"/kaggle/input/electoral-programs/it/elec_prog_extr_summ_version_2_top_10\" # top 10  electoral progams\n\ndf_programs = join_csv(input_dir, \"programs_by_category.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_programs = df_programs.reindex(index = ['PD','Movimento5Stelle','AzioneItaliaviva', 'ForzaItalia',  'Lega',  'FratellidItalia'])\ndf_programs.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arguments = df_programs.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = 'all-MiniLM-L6-v2' # use on the english text\nmodel = 'efederici/sentence-bert-base' # use on the italian text\ncm = similarity_matrix(df_programs, arguments, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting","metadata":{}},{"cell_type":"code","source":"plot_similarity_matrix(cm, \"General Program Similarity\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categories Analysis","metadata":{}},{"cell_type":"code","source":"plot_similarity_matrix_by_argument(df_programs, [\"Diritti\"], model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Most common words","metadata":{}},{"cell_type":"markdown","source":"##### Ambiente ","metadata":{}},{"cell_type":"code","source":"return_most_common_words_by_arguments(df_programs, [\"Ambiente\"], 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Diritti","metadata":{}},{"cell_type":"code","source":"plot_similarity_matrix_by_argument(df_programs, [\"Ambiente\"], model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return_most_common_words_by_arguments(df_programs, [\"Diritti\"], 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# most used words in all the category\nwith open(\"/kaggle/input/programs/program_by_index_version_2/PD/Diritti.txt\", \"r\") as f:\n    text = f.readlines()\ntext = \" \".join(text)\n\"Gay\" in most_common_words(text, 10000) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Esteri","metadata":{}},{"cell_type":"code","source":"return_most_common_words_by_arguments(df_programs, [\"Esteri\"], 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scatter Plot","metadata":{}},{"cell_type":"code","source":"plot_scatter_plot(df_programs, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}