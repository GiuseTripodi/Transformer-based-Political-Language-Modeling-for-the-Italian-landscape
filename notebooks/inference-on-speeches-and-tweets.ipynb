{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inference on Speeches and Tweets\n\n**Author:** [Giuseppe Tripodi](https://www.linkedin.com/in/giuseppe-tripodi-unical/)<br>\n**Date created:** 2022/11/12<br>\n**Description:** Predict model results on a test set consisting of speeches and tweets","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"## Install package","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers\n!pip install sentencepiece\n!pip install sacremoses\n!pip install nltk\n!pip install transformers\n!pip install evaluate\n!pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport csv\nimport re\nimport wandb\nimport transformers\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer\nfrom sklearn import preprocessing\nimport numpy as np\nimport evaluate\nfrom transformers.integrations import TensorBoardCallback\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer\nfrom transformers import Pipeline, TextClassificationPipeline\nimport numpy as np\nfrom datasets import load_dataset, load_metric\nimport pandas as pd\nimport torch\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom math import pi\nfrom datetime import date\nfrom sklearn.metrics import plot_confusion_matrix\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup Weight&Biases and General variables","metadata":{}},{"cell_type":"code","source":"today = date.today()\ntoday = today.strftime(\"%b-%d-%Y\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env WANDB_PROJECT=\n%env WANDB_LOG_MODEL=\n%env WANDB_API_KEY=","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Functions\n","metadata":{}},{"cell_type":"code","source":"def softmax(outputs):\n    maxes = np.max(outputs, axis=-1, keepdims=True)\n    shifted_exp = np.exp(outputs - maxes)\n    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TextClassification Pipeline","metadata":{}},{"cell_type":"code","source":"class MyTextClassificationPipeline(TextClassificationPipeline):\n    \"\"\"\n    Custom text classification pipeline\n    \"\"\"\n    def _sanitize_parameters(self, **kwargs):\n        \"\"\"\n        Checks the parameters passed. Returns three dict of kwargs\n        that will be passed to preprocess, _forward and postprocess.\n        :param kwargs: \n        :return: \n        \"\"\"\"\"\n        return {}, {}, {}\n\n    def preprocess(self, inputs):\n        \"\"\"\n        Takes the input and turn it into something feedable to the model\n        :param inputs:\n        :param maybe_arg:\n        :return:\n        \"\"\"\n        return self.tokenizer(inputs, return_tensors=self.framework)\n\n    def _forward(self, model_inputs):\n        \"\"\"\n        Forward step\n        :param model_inputs:\n        :return:\n        \"\"\"\n        return self.model(**model_inputs)\n\n    def postprocess(self, model_outputs):\n        \"\"\"\n        Turns the forward step output into the final output\n        :param model_outputs:\n        :return:\n        \"\"\"\n        logits = model_outputs.logits[0].numpy()\n        probabilities = softmax(logits)\n\n        best_class = np.argmax(probabilities)\n        label = self.model.config.id2label[best_class]\n        score = probabilities[best_class].item()\n        logits = logits.tolist()\n        return {\"label\": label, \"best_class_code\": best_class, \"score\": score, \"logits\": logits}","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compute Metrics","metadata":{}},{"cell_type":"code","source":"PLOT_PATH = \"./\"\n\nclass ComputeMetrics:\n    \"\"\"\n    A class used to compute metrics on model output and plot the results.\n    ...\n\n    Methods\n    ---------\n    compute_metrics()\n        returns the computed metrics\n        \n    plot_consistency_for_politician()\n        plots a bar pot of true positive and total prediction\n    \n    confusion_matrix_plot()\n        plots the confusion matrix\n    \n    misclassification_pie_chart()\n        draws a pie chart of false positives\n        \n\n    \"\"\"\n\n    def __init__(self, model_predictions, model_inputs, mapping, tc2=False, tags=\"\"):\n        \"\"\"\n        :param model_predictions: dict\n            output of the TextClassificationPipeline\n        :param model_inputs: DataFrame\n            model input\n        :param mapping: \n            mapping between label and associated id, used to map input labels to ids used by models\n        :parma tc2: boolean\n            This is true if you perform text classification on election programs. Input labels are different in text classification of election programs.\n        \"\"\"\n        self.model_predictions = model_predictions\n        self.model_inputs = model_inputs\n        self.mapping = mapping\n        self.tags = tags\n        self.tc2 = tc2\n\n        # DEFINE Y_PRED AND Y_TRUE\n        self.references_labels = self.model_inputs[\"label\"].map(mapping).tolist() # y_true\n        if self.tc2:\n            #  If tc2, the labels must be changed because \"CarloCalenda\" and \"MatteoRenzi\" have the same labels on the test set.\n            mapping_prediction_label = {\n                \"CarloCalenda\": self.mapping[\"TerzoPolo\"],\n                \"EnricoLetta\":self.mapping[\"PD\"],\n                \"GiorgiaMeloni\":self.mapping[\"FratelliDItalia\"],\n                \"GiuseppeConte\":self.mapping[\"Movimento5Stelle\"],\n                \"MatteoRenzi\":self.mapping[\"TerzoPolo\"],\n                \"MatteoSalvini\":self.mapping[\"Lega\"],\n                \"SilvioBerlusconi\":self.mapping[\"ForzaItalia\"]\n            }\n            self.predictions_labels = pd.DataFrame(self.model_predictions)[\"label\"].map(mapping_prediction_label).tolist() #y_pred\n        else: \n            self.predictions_labels = pd.DataFrame(self.model_predictions)[\"best_class_code\"].tolist() #y_pred\n\n    def compute_metrics(self):\n        \"\"\"\n        Prints the values of: Accuracy, F1, precision and recall\n        \"\"\"\n        # load and define the different metrics\n        accuracy = evaluate.load('accuracy')\n        f1 = evaluate.load('f1', average='macro')\n        precision = evaluate.load('precision')\n        recall = evaluate.load('recall', average='macro')\n        roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n        \n        # print metrics\n        print(accuracy.compute(predictions=self.predictions_labels, references=self.references_labels))\n        print(f1.compute(predictions=self.predictions_labels, references=self.references_labels, average='weighted'))\n        print(precision.compute(predictions=self.predictions_labels, references=self.references_labels, average='weighted'))\n        print(recall.compute(predictions=self.predictions_labels, references=self.references_labels, average='weighted'))\n\n        # ROC AUC\n        pred_scores = pd.DataFrame(self.model_predictions)[\"logits\"].transform(softmax)\n        try:\n            print(self.roc_auc_score.compute(references=self.references_labels, prediction_scores=pred_scores,multi_class='ovr', labels=[0, 1, 2, 3, 4, 5, 6]))\n        except:\n            pass\n\n    def plot_consistency_for_politician(self):\n        \"\"\"\n        plots a bar pot of true positive and total prediction number\n        \"\"\"\n        # compute the confusion matrix\n        matrix = confusion_matrix(self.references_labels, self.predictions_labels, labels=np.arange(len(self.mapping.keys())))\n        # takes only the TP\n        diagonal = matrix.diagonal()\n        # takes the number of predictions\n        tot_ele = []\n        for i in range(len(matrix)):\n            tot_ele.append(sum(matrix[i]))\n\n        # plot the results\n        politician = self.mapping.keys()\n        X_axis = np.arange(len(politician))\n\n        fig = plt.figure(figsize=(10, 5))\n        # creating the bar plot\n        plt.bar(X_axis - 0.2,diagonal, color=\"maroon\", width=0.4, label=\"Correct predictions\")\n        plt.bar(X_axis + 0.2,tot_ele, color=\"#E5BABA\", width=0.4, label=\"Total number of predictions\")\n\n        plt.xticks(X_axis, politician)        \n        plt.xlabel(\"Italian Politician\")\n        plt.ylabel(\"number of predictions\")\n        plt.title(\"Italian Politician Accuracy\", fontsize=12)\n        plt.legend()\n        plt.savefig(f\"{PLOT_PATH}/accuracy_for_politician_{'tc2' if self.tc2 else 'tc1'}_{self.tags}_{today}.png\")\n        \n        # print the percentage\n        for i in range(len(tot_ele)):\n            print(f\"Politico: {list(politician)[i]}\")\n            print(f\"predizioni corrette:{diagonal[i]}\\npredizioni totali: {tot_ele[i]}\")\n            print(f\"Accuracy: {diagonal[i] / tot_ele[i]}\")\n            print(\"\\n\")\n\n\n    def confusion_matrix_plot(self):\n        \"\"\"\n        Plots the confusion matrix\n        \"\"\"\n        disp = ConfusionMatrixDisplay.from_predictions(y_true=self.references_labels, y_pred= self.predictions_labels, labels=np.arange(len(self.mapping.keys())),\n                                                       display_labels=list(self.mapping.keys()), cmap=plt.cm.Reds)\n        fig = disp.ax_.get_figure()\n        disp.ax_.tick_params(axis='x', which='major', labelsize=13)\n        disp.ax_.tick_params(axis='y', which='major', labelsize=13)\n        fig.set_figwidth(15)\n        fig.set_figheight(10)\n        plt.xticks(rotation=30)\n        plt.title(\"Confusion Matrix\", fontsize=14)\n        plt.savefig(f\"{PLOT_PATH}/confusion_matrix_{'tc2' if self.tc2 else 'tc1'}_{self.tags}_{today}.png\")\n    \n\n\n\n    def misclassification_pie_chart(self):\n        y_pred = np.array(self.predictions_labels)\n        y_true = np.array(self.references_labels)\n\n        # takes only the misclassified element\n        y_pred_mis = y_pred[y_pred != y_true]\n        y_true_mis = y_true[y_pred != y_true]\n        matrix = confusion_matrix(y_true_mis, y_pred_mis, labels=np.arange(len(self.mapping.keys())))\n        politician_names = list(self.mapping.keys())\n\n        colors = plt.get_cmap('Blues')(np.linspace(0.2, 0.7, len(matrix[0])))\n        fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 15))\n        fig.tight_layout()\n        i = 0\n        for ax in axs.ravel():\n            if i < len(matrix):\n                ax.set_title(politician_names[i], fontsize=15)\n                ax.pie(matrix[i], colors=colors,\n                       labels=[politician_names[pol_name] if matrix[i][pol_name] != 0 else None for pol_name in\n                               range(len(politician_names))])\n                i += 1\n            else:\n                fig.delaxes(ax)\n                # last pie\n                ax.pie([1, 0, 0, 0, 0, 0, 0])\n        plt.savefig(f\"{PLOT_PATH}/politician_misclassification_{'tc2' if self.tc2 else 'tc1'}_{self.tags}_{today}.png\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def define_structure_for_line_plots(dataset_input, eval_predict) -> pd.DataFrame:\n    \"\"\"\n    creates the dataframe that will be used to generate all line graphs\n    :return: dataframe\n    \"\"\"\n    df = pd.DataFrame(eval_predict).drop([\"logits\", \"best_class_code\"], axis=1)\n    df = df.rename(columns={\"label\":\"assigned_label\"})\n        \n    #input dataframe\n    df_input = dataset_input.copy()\n    df_input[\"impression\"] = df_input[[\"viewCount\", \"likeCount\", \"commentCount\", \"retweet_count\" , \"reply_count\", \"quote_count\"]].sum(axis=1)\n    df_input.drop(df_input.columns.difference([\"video_id\", \"created_at\", \"text\" , \"label\" , \"tweet_id\", \"impression\"]), 1, inplace=True)\n    df_input.rename(columns={\"label\":\"original_label\"}, inplace=True)\n\n    # concat the dataframe\n    df_input = pd.concat([df_input, df], axis=1)\n    return df_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def line_plot_correlation_score_impression(df: pd.DataFrame):\n    fontsize = 12\n    fig, ax = plt.subplots(2,2, figsize=(20, 10))\n\n    # TEST: NORMALIZE THE IMPRESSION COLUMNS\n    # copy the data\n    df_min_max_scaled = df.copy()\n  \n    # apply normalization techniques on Column 1\n    column = 'impression'\n    df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())    \n    df = df_min_max_scaled.copy()\n    # END TEST\n    \n    # TWEETS PLOTTING\n    df_tweets = df.loc[df[\"tweet_id\"] != 0]    \n    # plotting tweets\n    ax[0][0].set_title(\"Tweets score impression correlation\", fontdict={\"fontsize\":fontsize})\n    sns.histplot(data=df_tweets, x=\"impression\", y=\"score\", cbar=True, bins=30, ax = ax[0][0])\n    \n    ax[0][1].set_title(\"Tweets impression Distributions\", fontdict={\"fontsize\":fontsize})\n    sns.kdeplot(data=df_tweets, x=\"impression\",  weights=\"score\", hue=\"original_label\" , ax=ax[0][1])    \n        \n    # SPEECH plotting\n    df_speech = df.loc[df[\"video_id\"] != \"0\"]\n    df_speech = df_speech.groupby([\"video_id\", \"original_label\"])[\"score\", \"impression\"].mean()    \n    # plotting\n    ax[1][0].set_title(\"Speech score impression correlation\", fontdict={\"fontsize\":fontsize})\n    sns.histplot(data=df_speech, x=\"impression\", y=\"score\",cbar=True, bins=30, ax = ax[1][0])\n    \n    ax[1][1].set_title(\"Speech impression Distributions\", fontdict={\"fontsize\":fontsize})\n    sns.kdeplot(data=df_speech, x=\"impression\",  weights=\"score\", hue=\"original_label\" , ax=ax[1][1])    \n    plt.savefig(f\"{PLOT_PATH}/score_impression_correlation_{today}.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def line_plot_correlation_score_period(df: pd.DataFrame):\n    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], format=\"%Y-%m-%d %H:%M:%S+00:00\")\n    fontsize = 12\n    fig, ax = plt.subplots(2,2, figsize=(20, 10))\n\n    \n    # TWEETS PLOTTING\n    df_tweets = df.loc[df[\"tweet_id\"] != 0]\n    df_tweets = df_tweets.groupby([df.created_at.dt.month_name().rename('month'), \"original_label\"])[\"score\", \"impression\"].mean()    \n    \n    # plotting tweets\n    ax[0][0].tick_params(labelrotation = 25)\n    ax[0][0].set_title(\"Tweets score period correlation\", fontdict={\"fontsize\":fontsize})\n    sns.histplot(data=df_tweets, x=\"month\", y=\"score\",cbar=True,  bins=30, ax = ax[0][0])\n    \n    ax[0][1].tick_params(labelrotation = 25)\n    ax[0][1].set_title(\"Tweets impression Distributions\", fontdict={\"fontsize\":fontsize})\n    hist = sns.histplot(data=df_tweets, x=\"month\",  weights=\"score\", multiple=\"dodge\", shrink=1, binwidth=5, hue=\"original_label\" , ax=ax[0][1])    \n    sns.move_legend(hist, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n    \n    # SPEECH PLOTTING\n    df_speech = df.loc[df[\"video_id\"] != \"0\"]\n    df_speech = df_speech.groupby([df.created_at.dt.month_name().rename('month'), \"original_label\"])[\"score\", \"impression\"].mean()    \n    \n    # plotting \n    ax[1][0].tick_params(labelrotation = 15)\n    ax[1][0].set_title(\"Speech score period correlation\", fontdict={\"fontsize\":fontsize})\n    sns.histplot(data=df_speech, x=\"month\", y=\"score\",cbar=True,  bins=30, ax = ax[1][0])\n    \n    ax[1][1].tick_params(labelrotation = 15)\n    ax[1][1].set_title(\"Speech impression Distributions\", fontdict={\"fontsize\":fontsize})\n    hist_s = sns.histplot(data=df_speech, x=\"month\",  weights=\"score\", multiple=\"dodge\", shrink=1, binwidth=5, hue=\"original_label\" , ax=ax[1][1])    \n    sns.move_legend(hist_s, bbox_to_anchor=(1.02, 0.5), loc='upper left', borderaxespad=0)\n    fig.tight_layout(pad=2.0)\n    plt.savefig(f\"{PLOT_PATH}/score_period_correlation_{today}.png\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference ","metadata":{}},{"cell_type":"markdown","source":"## Read test set","metadata":{}},{"cell_type":"code","source":"# Define some variables\nHOME = \"../input/models\"\nNUM_LABELS = 7\n\ndata = pd.read_csv(\"../input/text-classification-1/it/test_set.csv\")\n#data.head()\n\n# define the mapping between label and id for the text classification 1\nmapping = {\n            \"CarloCalenda\":0,\n            \"EnricoLetta\":1,\n            \"GiorgiaMeloni\":2,\n            \"GiuseppeConte\":3,\n            \"MatteoRenzi\":4,\n            \"MatteoSalvini\":5,\n            \"SilvioBerlusconi\":6\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load models","metadata":{}},{"cell_type":"code","source":"run = wandb.init()\nartifact = run.use_artifact('giusetrip98/ItalianPoliticianConsistency/gilberto_tc1_new_speech_and_tweets:v0', type='model')\n\nartifact_dir = artifact.download()\n\n#checkpoint = \"Musixmatch/umberto-wikipedia-uncased-v1\"\n#checkpoint = \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\"\ncheckpoint = \"idb-ita/gilberto-uncased-from-camembert\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Hugging Face model from that folder using the same model class\nmodel = AutoModelForSequenceClassification.from_pretrained(artifact_dir, num_labels=NUM_LABELS)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, padding=True, truncation=True)\nmy_pipeline = MyTextClassificationPipeline(model=model, tokenizer=tokenizer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compute Metrics and Plotting","metadata":{}},{"cell_type":"code","source":"# predict\neval_predict = my_pipeline(data[\"text\"].tolist())\n#print(eval_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metric computation\ncm = ComputeMetrics(eval_predict, data, mapping, tc2=False)\ncm.compute_metrics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Consistency for politician","metadata":{}},{"cell_type":"code","source":"cm.plot_consistency_for_politician()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"matrix = cm.confusion_matrix_plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Misclassification Pie Chart","metadata":{}},{"cell_type":"code","source":"cm.misclassification_pie_chart()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Score and Impression","metadata":{}},{"cell_type":"code","source":"df = define_structure_for_line_plots(data, eval_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute accuracy for type\ndf_video = df[df[\"video_id\"] != '0']\ndf_tweets = df[df[\"tweet_id\"] != 0]\n\nfor pol in df[\"original_label\"].unique():\n    print(f\"Pol: {pol}\")\n    TP_video  = len(df_video[df_video['assigned_label'] == df_video['original_label']][df_video['assigned_label'] == pol])\n    tot_video = len(df_video[df_video['original_label'] == pol])\n    \n    TP_tweets  = len(df_tweets[df_tweets['assigned_label'] == df_tweets['original_label']][df_tweets['assigned_label'] == pol])\n    tot_tweets = len(df_tweets[df_tweets['original_label'] == pol])\n    print(f\"tot_tp = {TP_tweets + TP_video}\")\n    print(f\"tot = {tot_video + tot_tweets}\")\n    print(f\"Acc video: {TP_video/tot_video}\")\n    print(f\"Acc tweets: {TP_tweets/tot_tweets}\")\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute avarage score of misclassification for speeches\ndf_mis_speeches = df[(df[\"original_label\"] != df[\"assigned_label\"]) & (df[\"video_id\"] != '0')]\ngrouped_single_speeches = df_mis_speeches.groupby('original_label').agg({'score': ['mean', 'max', 'min', 'count']})\n\nprint(grouped_single_speeches)\nprint()\nprint(grouped_single_speeches[\"score\"][\"count\"] / (grouped_single_speeches[\"score\"][\"count\"] + grouped_single_tweets[\"score\"][\"count\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute avarage score of misclassification for tweets\ndf_mis_tweets = df[(df[\"original_label\"] != df[\"assigned_label\"]) & (df[\"tweet_id\"] != 0)]\ngrouped_single_tweets = df_mis_tweets.groupby('original_label').agg({'score': ['mean', 'max', 'min', 'count']})\n\nprint(grouped_single_tweets)\nprint()\nprint(grouped_single_tweets[\"score\"][\"count\"] / (grouped_single_speeches[\"score\"][\"count\"] + grouped_single_tweets[\"score\"][\"count\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"line_plot_correlation_score_impression(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Score and Time","metadata":{}},{"cell_type":"code","source":"line_plot_correlation_score_period(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}