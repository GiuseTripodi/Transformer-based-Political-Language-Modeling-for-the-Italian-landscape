{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune Models\n\n**Author:** [Giuseppe Tripodi](https://www.linkedin.com/in/giuseppe-tripodi-unical/)<br>\n**Date created:** 2022/11/12<br>\n**Description:** Fine-tuning the models on the training set defined for the type of text classification analysis","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"## Install package","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers\n!pip install sentencepiece\n!pip install sacremoses\n!pip install nltk\n!pip install transformers\n! pip install evaluate\n!pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport csv\nimport re\nimport wandb\nimport transformers\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer,EarlyStoppingCallback\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer\nfrom sklearn import preprocessing\nimport numpy as np\nimport evaluate\nfrom transformers.integrations import TensorBoardCallback\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup Weight&Biases","metadata":{}},{"cell_type":"code","source":"#os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env WANDB_PROJECT=\n%env WANDB_LOG_MODEL=\n%env WANDB_API_KEY=","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Functions\n","metadata":{}},{"cell_type":"code","source":"#CONSTANT\nNUM_LABELS = 7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FineTuneModel:\n    def __init__(self, checkpoint, tags, dataset, output_dir, batch_size=32, learning_rate=2e-5, num_epochs=5):\n        self.checkpoint = checkpoint\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.output_dir = output_dir\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.tags = tags\n\n\n        #define the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n        self.label_encoder = preprocessing.LabelEncoder()\n        #get the mapping id to label\n        self.label_encoder.fit(self.dataset[\"train\"][\"label\"])\n        self.label2id = None\n        self.id2label = None\n\n        # load metric\n        self.metric = evaluate.load(\"accuracy\", \"precision\")\n\n    def compute_id2labels(self):\n        \"\"\"\n        Computes the variable id2label and label2id used during the training\n        :return: \n        \"\"\"\n        transformed_array = self.label_encoder.transform(self.label_encoder.classes_)\n        # convert the id from int64 to int\n        id_ = [transformed_array[i].item() for i in range(len(transformed_array))]\n        self.label2id = dict(zip(self.label_encoder.classes_, id_))\n        self.id2label = dict(zip(id_, self.label_encoder.classes_))\n\n    def preprocess_function(self, examples):\n        \"\"\"\n        Preprocessing function, tokenize text and truncate sequences\n        to be no longer than model’s maximum input length.\n\n        :return:\n        \"\"\"\n        # transform non-numerical labels to numerical labels.\n        examples[\"label\"] = self.label_encoder.transform(examples[\"label\"])\n        return self.tokenizer(examples[\"text\"], truncation=True, padding=True)\n    \n    def compute_metrics(self, eval_pred):\n        \"\"\"\n        Computes the metrics between the labels and the output of the predictions\n        :param eval_pred:\n        :return:\n        \"\"\"\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return self.metric.compute(predictions=predictions, references=labels)\n\n    def fine_tune(self):\n        # Preprocessing\n        self.compute_id2labels()\n\n        encoded_dataset = self.dataset.map(self.preprocess_function, batched=True)\n        # define config\n        config = AutoConfig.from_pretrained(self.checkpoint, label2id=self.label2id, id2label=self.id2label,  num_labels=NUM_LABELS)\n        # load model with config\n        model = AutoModelForSequenceClassification.from_pretrained(self.checkpoint, config=config, ignore_mismatched_sizes=True)\n\n        # define train argument\n        model_name = self.checkpoint.split(\"/\")[-1]\n        \n        strategy = \"steps\"\n        interval_steps = 20\n\n        args = TrainingArguments(\n            output_dir=self.output_dir + f\"/{model_name}-finetuned-{self.tags}\",\n                \n            save_strategy = \"epoch\",\n            evaluation_strategy = \"epoch\",\n            logging_strategy = strategy,\n            \n               \n            save_steps = interval_steps,\n            logging_steps = interval_steps,\n            eval_steps = interval_steps,\n            \n            per_device_train_batch_size=self.batch_size,\n            per_device_eval_batch_size=self.batch_size,\n            num_train_epochs=self.num_epochs,\n            report_to=\"wandb\",\n            weight_decay=0.01,\n            learning_rate=self.learning_rate,\n\n            load_best_model_at_end=True\n        )\n\n        # define the trainer\n        trainer = Trainer(\n            model,\n            args,\n            train_dataset=encoded_dataset[\"train\"],\n            eval_dataset=encoded_dataset[\"validation\"],\n            tokenizer=self.tokenizer,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=5), TensorBoardCallback() ], \n            compute_metrics = self.compute_metrics\n        )\n\n        #trainer.train(resume_from_checkpoint=\"../input/models/bert-base-uncased-finetuned-textClass1/checkpoint-948\") # only if continue training from a checkpoint\n        trainer.train()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tune Models - Text Classification on Speeches and Tweets","metadata":{}},{"cell_type":"markdown","source":"## Define variable","metadata":{}},{"cell_type":"code","source":"# Define variables\nHOME = \"../input\"    \ntags = \"_tc1_test_più_epoche\"\n\npath_set = f\"{HOME}/text-classification-1/it\"\ndata_files = {\n        \"train\": f\"{path_set}/train_set.csv\", \n        \"validation\": f\"{path_set}/val_set.csv\",\n        \"test\": f\"{path_set}/test_set.csv\"\n}\noutput_dir = f\"./models\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training paramether","metadata":{}},{"cell_type":"code","source":"batch_size = 32\nnum_epochs = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the datasets","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"csv\",data_files=data_files)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UmBERTo, Alberto and Gilberto","metadata":{}},{"cell_type":"code","source":"\n#checkpoint = \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\" # Alberto\ncheckpoint = \"idb-ita/gilberto-uncased-from-camembert\" # Gilberto\n#checkpoint = \"Musixmatch/umberto-wikipedia-uncased-v1\" # Umberto\nrun = wandb.init(project=\"ItalianPoliticianConsistency\", reinit=True, name=checkpoint+tags)\n\nmodel = FineTuneModel(checkpoint=checkpoint, tags=tags, dataset=dataset, batch_size=batch_size, output_dir=output_dir, num_epochs=num_epochs)\nmodel.fine_tune()\n\nrun.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT, Distil-Bert","metadata":{}},{"cell_type":"code","source":"\"\"\"\n#checkpoint = \"distilbert-base-uncased\"\ncheckpoint = \"bert-base-uncased\"\nrun = wandb.init(project=\"ItalianPoliticianConsistency\", reinit=True, name=checkpoint+tags)\n\nmodel = FineTuneModel(checkpoint=checkpoint, tags=tags, dataset=dataset, batch_size=batch_size, output_dir=output_dir, num_epochs=num_epochs)\nmodel.fine_tune()\n\nrun.finish()\n\"\"\"","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tune Models - Text Classification on electoral programs","metadata":{}},{"cell_type":"markdown","source":"## Define variable","metadata":{}},{"cell_type":"code","source":"# Define some variables\nHOME = \"../input\"    \ntags = \"_tc2_test_più_epoche\"\n\npath_set = f\"{HOME}/text-classification-2/it\"\ndata_files = {\n        \"train\": f\"{path_set}/train_set.csv\", \n        \"validation\": f\"{path_set}/val_set.csv\"\n}\noutput_dir = f\"./models\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nnum_epochs = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load dataset\ndataset = load_dataset(\"csv\",data_files=data_files)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UmBERTo, Alberto and Gilberto","metadata":{}},{"cell_type":"code","source":"\"\"\"\n#checkpoint = \"Musixmatch/umberto-wikipedia-uncased-v1\" # Umberto\n#checkpoint = \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\" # Alberto\ncheckpoint = \"idb-ita/gilberto-uncased-from-camembert\" # Gilberto\n\nrun = wandb.init(project=\"ItalianPoliticianConsistency\", reinit=True, name=checkpoint+tags)\n\nmodel = FineTuneModel(checkpoint=checkpoint, tags=tags, dataset=dataset, batch_size=batch_size, output_dir=output_dir, num_epochs=num_epochs)\nmodel.fine_tune()\n\nrun.finish()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}