{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inference on Electoral Program\n\n**Author:** [Giuseppe Tripodi](https://www.linkedin.com/in/giuseppe-tripodi-unical/)<br>\n**Date created:** 2022/11/12<br>\n**Description:** Predict model results on a test set consisting of electoral programs","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"## Install package","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers\n!pip install sentencepiece\n!pip install sacremoses\n!pip install nltk\n!pip install transformers\n!pip install evaluate\n!pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport csv\nimport re\nimport wandb\nimport transformers\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, \\\n    EarlyStoppingCallback\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer\nfrom sklearn import preprocessing\nimport numpy as np\nimport evaluate\nfrom transformers.integrations import TensorBoardCallback\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n#load dataset\nfrom datasets import load_dataset, load_metric\n#tockenizer\nfrom transformers import AutoTokenizer\nfrom transformers import Pipeline, TextClassificationPipeline\nimport numpy as np\nfrom datasets import load_dataset, load_metric\nimport pandas as pd\nimport torch\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom math import pi\nfrom datetime import date\nfrom sklearn.metrics import plot_confusion_matrix\nfrom os import path, listdir\nfrom os.path import isfile, join\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup Weight&Biases and General variables","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"today = date.today()\ntoday = today.strftime(\"%b-%d-%Y\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env WANDB_PROJECT=\n%env WANDB_LOG_MODEL=\n%env WANDB_API_KEY=","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Functions\n","metadata":{}},{"cell_type":"code","source":"def softmax(outputs):\n    maxes = np.max(outputs, axis=-1, keepdims=True)\n    shifted_exp = np.exp(outputs - maxes)\n    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def join_csv(input_dir, output_name):\n    \"\"\"\n    takes the csv in input_dir and concatenates them\n    \"\"\"\n    files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n    li = []\n    indexes = []\n    for file in files:\n        df = pd.read_csv(join(input_dir, file), index_col=None, header=0)\n        index = file[:file.index(\".\")]\n        li.append(df)\n        indexes.append(index)\n\n    frame = pd.concat(li, axis=0, ignore_index=True)\n    frame[\"indexes\"] = indexes\n    frame.set_index(\"indexes\", inplace=True)\n    \n    #save the output as a csv file\n    frame.to_csv(output_name)\n    \n    return frame","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TextClassification Pipeline","metadata":{}},{"cell_type":"code","source":"class MyTextClassificationPipeline(TextClassificationPipeline):\n    \"\"\"\n    Custom text classification pipeline\n    \"\"\"\n    def _sanitize_parameters(self, **kwargs):\n        \"\"\"\n        checks the parameters passed. Returns three dict of kwargs\n        that will be passed to preprocess, _forward and postprocess.\n        :param kwargs: \n        :return: \n        \"\"\"\"\"\n        return {}, {}, {}\n\n    def preprocess(self, inputs):\n        \"\"\"\n        Takes the input and turn it into something feedable to the model\n        :param inputs:\n        :param maybe_arg:\n        :return:\n        \"\"\"\n        return self.tokenizer(inputs, return_tensors=self.framework)\n\n    def _forward(self, model_inputs):\n        \"\"\"\n        forward step\n        :param model_inputs:\n        :return:\n        \"\"\"\n        return self.model(**model_inputs)\n\n    def postprocess(self, model_outputs):\n        \"\"\"\n        turns the output of the forward step into the final output\n        :param model_outputs:\n        :return:\n        \"\"\"\n        logits = model_outputs.logits[0].numpy()\n        probabilities = softmax(logits)\n\n        best_class = np.argmax(probabilities)\n        label = self.model.config.id2label[best_class]\n        score = probabilities[best_class].item()\n        logits = logits.tolist()\n        return {\"label\": label, \"best_class_code\": best_class, \"score\": score, \"logits\": logits}","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compute Metrics","metadata":{}},{"cell_type":"code","source":"PLOT_PATH = \"./\"\nNUM_LABELS_plot = 6\nclass ComputeMetrics:\n    \"\"\"\n    A class used to compute metrics on model output and plot the results.\n    ...\n\n    Methods\n    ---------\n    compute_metrics()\n        returns the computed metrics\n        \n    plot_consistency_for_politician()\n        plots a bar pot of true positive and total prediction\n    \n    confusion_matrix_plot()\n        plots the confusion matrix\n    \n    misclassification_pie_chart()\n        draws a pie chart of false positives\n        \n\n    \"\"\"\n    def __init__(self, model_predictions, model_inputs, mapping, tc2=False, tags=\"\"):\n        \"\"\"\n        :param model_predictions: dict\n            output of the TextClassificationPipeline\n        :param model_inputs: DataFrame\n            model input\n        :param mapping: \n            mapping between label and associated id, used to map input labels to ids used by models\n        :parma tc2: boolean\n            This is true if you perform text classification on election programs. Input labels are different in text classification of election programs.\n        \"\"\"\n        self.model_predictions = model_predictions\n        self.model_inputs = model_inputs\n        self.tags = tags\n        self.mapping = mapping\n        self.tc2 = tc2\n\n\n        # DEFINE Y_PRED AND Y_TRUE\n        self.references_labels = self.model_inputs[\"label\"].map(mapping).tolist() # y_true\n        if self.tc2:\n            #  If tc2, the labels must be changed because \"CarloCalenda\" and \"MatteoRenzi\" have the same labels on the test set.\n            mapping_prediction_label = {\n                \"CarloCalenda\": self.mapping[\"AzioneItaliaviva\"],\n                \"EnricoLetta\":self.mapping[\"PD\"],\n                \"GiorgiaMeloni\":self.mapping[\"FratellidItalia\"],\n                \"GiuseppeConte\":self.mapping[\"Movimento5Stelle\"],\n                \"MatteoRenzi\":self.mapping[\"AzioneItaliaviva\"],\n                \"MatteoSalvini\":self.mapping[\"Lega\"],\n                \"SilvioBerlusconi\":self.mapping[\"ForzaItalia\"]\n            }\n            self.predictions_labels = pd.DataFrame(self.model_predictions)[\"label\"].map(mapping_prediction_label).tolist()\n        else: \n            self.predictions_labels = pd.DataFrame(self.model_predictions)[\"best_class_code\"].tolist() #y_pred\n\n            \n            \n    def compute_metrics(self):  \n        \"\"\"\n        Prints the values of: Accuracy, F1, precision and recall\n        \"\"\"\n        # load and define the different metrics\n        accuracy = evaluate.load('accuracy')\n        f1 = evaluate.load('f1', average='macro')\n        precision = evaluate.load('precision')\n        recall = evaluate.load('recall', average='macro')\n        roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n\n        # print metrics\n        print(accuracy.compute(predictions=self.predictions_labels, references=self.references_labels))\n        print(f1.compute(predictions=self.predictions_labels, references=self.references_labels, average='weighted'))\n        print(precision.compute(predictions=self.predictions_labels, references=self.references_labels, average='weighted'))\n        print(recall.compute(predictions=self.predictions_labels, references=self.references_labels, average='weighted'))\n\n        # ROC AUC\n        pred_scores = pd.DataFrame(self.model_predictions)[\"logits\"].transform(softmax)\n        try:\n            print(self.roc_auc_score.compute(references=self.references_labels, prediction_scores=pred_scores,multi_class='ovr', labels=[0, 1, 2, 3, 4, 5, 6]))\n        except:\n            pass\n        \n    \n    def plot_consistency_for_politician(self):\n        \"\"\"\n        plots a bar pot of true positive and total prediction number\n        \"\"\"\n        # compute the confusion matrix\n        matrix = confusion_matrix(self.references_labels, self.predictions_labels, labels=np.arange(len(self.mapping.keys())))\n        # takes only the TP\n        diagonal = matrix.diagonal()\n        # takes the number of predictions\n        tot_ele = []\n        for i in range(len(matrix)):\n            tot_ele.append(sum(matrix[i]))\n        \n        #plot the results\n        politician = self.mapping.keys()\n        X_axis = np.arange(len(politician))\n\n        fig = plt.figure(figsize=(10, 5))\n        #creating the bar plot\n        plt.bar(X_axis - 0.2,diagonal, color=\"maroon\", width=0.4, label=\"Correct predictions\")\n        plt.bar(X_axis + 0.2,tot_ele, color=\"#E5BABA\", width=0.4, label=\"Total number of predictions\")\n\n        plt.xticks(X_axis, politician)\n        plt.xlabel(\"Electoral programs\")\n        plt.ylabel(\"number of predictions\")\n        plt.title(\"Italian Electoral Programs Accuracy\", fontsize=12)\n        plt.legend()\n        plt.savefig(f\"{PLOT_PATH}/accuracy_for_politician_{'tc2' if self.tc2 else 'tc1'}_{self.tags}_{today}.png\")\n        \n        # print the percentage\n        for i in range(len(tot_ele)):\n            print(f\"Politico: {list(politician)[i]}\")\n            print(f\"predizioni corrette:{diagonal[i]}\\npredizioni totali: {tot_ele[i]}\")\n            print(f\"Accuracy: {diagonal[i] / tot_ele[i]}\")\n            print(\"\\n\")\n    \n    def confusion_matrix_plot(self):\n        \"\"\"\n        Plots the confusion matrix\n        \"\"\"\n        disp = ConfusionMatrixDisplay.from_predictions(y_true=self.references_labels, y_pred= self.predictions_labels, labels=np.arange(len(self.mapping.keys())), display_labels=list(self.mapping.keys()), cmap=plt.cm.Blues)\n        fig = disp.ax_.get_figure()\n        fig.set_figwidth(15)\n        fig.set_figheight(10)\n        plt.title(\"Confusion Matrix\", fontsize=14)\n        plt.savefig(f\"{PLOT_PATH}/confusion_matrix_{'tc2' if self.tc2 else 'tc1'}_{self.tags}_{today}.png\")\n\n    def misclassification_pie_chart(self):\n        y_pred = np.array(self.predictions_labels)\n        y_true = np.array(self.references_labels)\n\n        # takes only the misclassified element\n        y_pred_mis = y_pred[y_pred != y_true]\n        y_true_mis = y_true[y_pred != y_true]\n        matrix = confusion_matrix(y_true_mis, y_pred_mis, labels=np.arange(len(self.mapping.keys())))\n        politician_names = list(self.mapping.keys())\n\n        colors = plt.get_cmap('Blues')(np.linspace(0.2, 0.7, len(matrix[0])))\n        fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 15))\n        fig.tight_layout()\n        i = 0\n        for ax in axs.ravel():\n            if i < len(matrix):\n                ax.set_title(politician_names[i], fontsize=15)\n                label = [politician_names[pol_name] if matrix[i][pol_name] != 0 else None for pol_name in range(len(politician_names))]\n                ax.pie(matrix[i], colors=colors,labels=label)\n                i += 1\n            else:\n                # last pie\n                ax.pie([1, 0, 0, 0, 0, 0, 0])\n        plt.savefig(f\"{PLOT_PATH}/politician_misclassification_{'tc2' if self.tc2 else 'tc1'}_{self.tags}_{today}.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def define_structure_for_line_plots(dataset_input, eval_predict) -> pd.DataFrame:\n    \"\"\"\n    creates the dataframe that will be used to generate all line graphs\n    :return: dataframe\n    \"\"\"\n    df = pd.DataFrame(eval_predict).drop([\"logits\", \"best_class_code\"], axis=1)\n    df[\"label\"] = df[\"label\"].map({\n            \"CarloCalenda\":\"AzioneItaliaviva\",\n            \"MatteoRenzi\":\"AzioneItaliaviva\",\n            \"EnricoLetta\":\"PD\",\n            \"GiorgiaMeloni\":\"FratellidItalia\",\n            \"GiuseppeConte\":\"Movimento5Stelle\",\n            \"MatteoSalvini\":\"Lega\",\n            \"SilvioBerlusconi\":\"ForzaItalia\"\n        })\n    df = df.rename(columns={\"label\":\"assigned_label\"})\n    #modified dataframe in input\n    df_input = dataset_input.copy()\n    df_input.rename(columns={\"label\":\"original_label\"}, inplace=True)\n        \n    # concat the dataframe\n    df_input = pd.concat([df_input, df], axis=1)\n    return df_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_bar_category_score_by_program(df: pd.DataFrame,title:str, tags ):\n    fig, ax = plt.subplots(figsize=(15,8))\n    ax.set_title(title, fontsize=15)\n    sns.barplot(data=df, x=\"category\", y=\"score\", hue=\"original_label\", ax=ax)\n    plt.xticks(rotation=30)\n    plt.savefig(f\"{PLOT_PATH}/{title}_{tags}_{today}.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference: Programs divided by Category","metadata":{}},{"cell_type":"markdown","source":"## Read test set","metadata":{}},{"cell_type":"code","source":"df_programs = pd.read_csv(\"/kaggle/input/text-classification-2/it/programs_by_index_by_nltk.csv\")\narguments = df_programs.columns\n\n# define the mapping between label and id for the text classification 2\nmapping = {\n            \"AzioneItaliaviva\":0,\n            \"PD\":1,\n            \"FratellidItalia\":2,\n            \"Movimento5Stelle\":3,\n            \"Lega\":4,\n            \"ForzaItalia\":5\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_programs.label.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_programs.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download Model","metadata":{}},{"cell_type":"code","source":"run = wandb.init()\n#artifact = run.use_artifact('giusetrip98/ItalianPoliticianConsistency/alberto_tc2_new_speech_and_tweets:v0', type='model')\nartifact = run.use_artifact('giusetrip98/ItalianPoliticianConsistency/alberto_tc2_new_speech_and_tweets:v0', type='model')\n#artifact = run.use_artifact('giusetrip98/ItalianPoliticianConsistency/gilberto_tc2_new_speech_and_tweets:v0', type='model')\n\n#TEST \n#END TEST\nartifact_dir = artifact.download()\n\n#checkpoint = \"Musixmatch/umberto-wikipedia-uncased-v1\"\n#checkpoint = \"idb-ita/gilberto-uncased-from-camembert\"\ncheckpoint = \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Hugging Face model from that folder using the same model class\nNUM_LABELS = 7\nmodel = AutoModelForSequenceClassification.from_pretrained(artifact_dir, num_labels=NUM_LABELS)\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, padding=True, truncation=False)\nmy_pipeline = MyTextClassificationPipeline(model=model, tokenizer=tokenizer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction and Plotting","metadata":{}},{"cell_type":"code","source":"#df_programs.drop([1150, 2908], inplace=True)\n\"\"\"\n# Test to find the problem\nl = df_programs[\"text\"].tolist()\nfor i in range(len(l)):\n    print(f\"last computed {i}: {l[i][:12]}\")\n    my_pipeline(l[i])\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_predict = my_pipeline(df_programs[\"text\"].tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = df_programs.iloc[0][\"text\"]\ntest\nmy_pipeline(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metric computation\ntags = \"by_index_extraction2\"\ncm = ComputeMetrics(eval_predict, df_programs, mapping, tc2=True, tags=tags)\nprint(cm.compute_metrics())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm.plot_consistency_for_politician()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm.confusion_matrix_plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm.misclassification_pie_chart()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis by category","metadata":{}},{"cell_type":"code","source":"tags = \"by_category_extraction2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define the new structure \ndf_index = define_structure_for_line_plots(df_programs, eval_predict)\ndf_index.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy Analysis for category","metadata":{}},{"cell_type":"code","source":"for_pol = df_index.groupby([\"original_label\", \"category\"]).count()\nfor_pol[\"TP\"] = df_index[df_index['assigned_label'] == df_index['original_label']].groupby([\"original_label\", \"category\"]).agg({\"score\": \"count\"})\nfor_pol[\"Acc\"] = for_pol[\"TP\"] / for_pol[\"score\"] \nfor_pol[\"Acc\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"general = df_index.groupby(by=\"category\").agg({\"score\": \"count\"})\ngeneral[\"TP\"] = df_index[df_index['assigned_label'] == df_index['original_label']].groupby(by=\"category\").agg({\"score\": \"count\"})\ngeneral[\"acc\"] = general[\"TP\"] / general[\"score\"]\ngeneral","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Score Analysis for category","metadata":{}},{"cell_type":"code","source":"title= \"Prediction Score by Category\"\nplot_bar_category_score_by_program(df_index,title, tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Misclassified Elements","metadata":{}},{"cell_type":"code","source":"# plot the misclassified element score\ndf_index_mis = df_index[df_index[\"original_label\"] != df_index[\"assigned_label\"]]\ndf_index_mis.to_csv(\"dataframe_con_solo_misclassificazioni.csv\")\nprint(df_index_mis.info())\ntitle= \"Prediction Score by Category - Misclassified Element\"\nplot_bar_category_score_by_program(df_index_mis, title, tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute avarage score of misclassification\ngrouped_single_tweets = df_index_mis.groupby('original_label').agg({'score': ['mean', 'max', 'min', 'count']})\nprint(grouped_single_tweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correct Predictions","metadata":{}},{"cell_type":"code","source":"# plot the correct element score\ndf_index_corr = df_index[df_index[\"original_label\"] == df_index[\"assigned_label\"]]\nprint(df_index_corr.info())\ntitle= \"Prediction Score by Category - True Positive\"\nplot_bar_category_score_by_program(df_index_corr, title, tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute avarage score of correct predictions\ngrouped_single_tweets = df_index_corr.groupby('original_label').agg({'score': ['mean', 'max', 'min', 'count']})\nprint(grouped_single_tweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}